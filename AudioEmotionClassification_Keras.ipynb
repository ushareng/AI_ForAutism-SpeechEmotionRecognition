{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00f358dbbd2f44b9aef40d69fddbefa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_045ed57ca15646c3b280c2d8b1160516",
              "IPY_MODEL_02e0f876c7e142d6a40fc1feedf170b5",
              "IPY_MODEL_28bfc6f85fe748d7abc47d7552ced244"
            ],
            "layout": "IPY_MODEL_57c44d2ffee841f8b20d61fd0bf1fa92"
          }
        },
        "045ed57ca15646c3b280c2d8b1160516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12d72777557d436cbaff00efc07bca6e",
            "placeholder": "​",
            "style": "IPY_MODEL_ccee9901a9d549d2ade6f7ba2d6540c3",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "02e0f876c7e142d6a40fc1feedf170b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff10dc490ac44294adc10e981920e975",
            "max": 380267417,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_623e1171e4be41ad856bd6e99c372b0d",
            "value": 380267417
          }
        },
        "28bfc6f85fe748d7abc47d7552ced244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4cc8c8105744ae1bf70fad08d152782",
            "placeholder": "​",
            "style": "IPY_MODEL_61edb7628cbe464499b354d5fd2f47e5",
            "value": " 380M/380M [00:03&lt;00:00, 84.1MB/s]"
          }
        },
        "57c44d2ffee841f8b20d61fd0bf1fa92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12d72777557d436cbaff00efc07bca6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccee9901a9d549d2ade6f7ba2d6540c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff10dc490ac44294adc10e981920e975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623e1171e4be41ad856bd6e99c372b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4cc8c8105744ae1bf70fad08d152782": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61edb7628cbe464499b354d5fd2f47e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the required libraries"
      ],
      "metadata": {
        "id": "-9EBj3Ahu4kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git datasets huggingface-hub joblib librosa resampy"
      ],
      "metadata": {
        "id": "wGVMvkI5oEcm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d5e9b2-b5dd-4587-ccea-9838e704673f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the data from the cloud storage"
      ],
      "metadata": {
        "id": "9c9M7WEsu8jg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nn5B_tEf4MH",
        "outputId": "798246c5-2752-49ab-a28e-55b883ded7b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-19 06:35:29--  https://storage.googleapis.com/kerascvnlp_data/archive.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.170.128, 142.251.175.128, 172.253.118.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.170.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 450102890 (429M) [application/zip]\n",
            "Saving to: ‘archive.zip’\n",
            "\n",
            "archive.zip         100%[===================>] 429.25M  17.4MB/s    in 25s     \n",
            "\n",
            "2023-07-19 06:35:54 (17.5 MB/s) - ‘archive.zip’ saved [450102890/450102890]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/kerascvnlp_data/archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/archive.zip -d ravdess/"
      ],
      "metadata": {
        "id": "IWLfxM30ghpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n"
      ],
      "metadata": {
        "id": "6AY-xEE5pO5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the librosa library to read the audio files and generate dataloader"
      ],
      "metadata": {
        "id": "hDn9JnCmvCqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature(file_path: str, mfcc_len: int = 39, mean_signal_length: int = 110000):\n",
        "    signal, fs = librosa.load(file_path)\n",
        "    s_len = len(signal)\n",
        "\n",
        "    if s_len < mean_signal_length:\n",
        "        pad_len = mean_signal_length - s_len\n",
        "        pad_rem = pad_len % 2\n",
        "        pad_len //= 2\n",
        "        signal = np.pad(signal, (pad_len, pad_len + pad_rem), 'constant', constant_values = 0)\n",
        "    else:\n",
        "        pad_len = s_len - mean_signal_length\n",
        "        pad_len //= 2\n",
        "        signal = signal[pad_len:pad_len + mean_signal_length]\n",
        "    mfcc = librosa.feature.mfcc(y=signal, sr=fs, n_mfcc=39)\n",
        "    mfcc = mfcc.T\n",
        "    feature = mfcc\n",
        "    return feature"
      ],
      "metadata": {
        "id": "fDM3L2sVpJtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from tqdm import tqdm\n",
        "lst = []\n",
        "path = '/content/ravdess'\n",
        "\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "        try:\n",
        "            X, sample_rate = librosa.load(os.path.join(subdir, file),\n",
        "                                          res_type='kaiser_fast')\n",
        "            #\n",
        "            file = int(file[7:8]) - 1\n",
        "            arr = X[:64000], file\n",
        "            lst.append(arr)\n",
        "        except ValueError as err:\n",
        "            print(err)\n",
        "            continue\n",
        "\n",
        "X, y = zip(*lst)\n",
        "X, y = np.asarray(X), np.asarray(y)"
      ],
      "metadata": {
        "id": "ChIND7E7gnRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d605b117-a18a-41de-ef90-1cd62f5826a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 55s, sys: 2.44 s, total: 2min 57s\n",
            "Wall time: 3min 17s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape,y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oWueG6nEkkB",
        "outputId": "25d28702-357c-4df1-aa6f-ebd6338eefd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2880, 64000), (2880,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_DURATION = 2\n",
        "# Sampling rate is the number of samples of audio recorded every second\n",
        "SAMPLING_RATE = 16000\n",
        "BATCH_SIZE = 2  # Batch-size for training and evaluating our model.\n",
        "NUM_CLASSES = 8  # Number of classes our dataset will have (11 in our case).\n",
        "HIDDEN_DIM = 768  # Dimension of our model output (768 in case of Wav2Vec 2.0 - Base).\n",
        "MAX_SEQ_LENGTH = MAX_DURATION * SAMPLING_RATE  # Maximum length of the input audio file.\n",
        "# Wav2Vec 2.0 results in an output frequency with a stride of about 20ms.\n",
        "MAX_FRAMES = 99\n",
        "MAX_EPOCHS = 5  # Maximum number of training epochs.\n",
        "\n",
        "MODEL_CHECKPOINT = \"facebook/wav2vec2-base\""
      ],
      "metadata": {
        "id": "Yc5b0U7Gswtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RAVDESS_CLASS_LABELS = (\"angry\", \"calm\", \"disgust\", \"fear\", \"happy\", \"neutral\",\"sad\",\"surprise\")"
      ],
      "metadata": {
        "id": "ReB88rT6iE8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = RAVDESS_CLASS_LABELS\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label\n",
        "\n",
        "print(id2label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ7NxEsduX-T",
        "outputId": "b4191333-a47a-4f2c-99df-2b5e6426203f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'0': 'angry', '1': 'calm', '2': 'disgust', '3': 'fear', '4': 'happy', '5': 'neutral', '6': 'sad', '7': 'surprise'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the feature extractor model from transformer library to extract features from the audio data"
      ],
      "metadata": {
        "id": "tuFJkJGJvcQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoFeatureExtractor\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "    MODEL_CHECKPOINT, return_attention_mask=True\n",
        ")\n",
        "\n",
        "\n",
        "audio_arrays = X\n",
        "inputs = feature_extractor(\n",
        "    audio_arrays,\n",
        "    sampling_rate=feature_extractor.sampling_rate,\n",
        "    max_length=MAX_SEQ_LENGTH,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFYtlDZJ2FRT",
        "outputId": "c6abd95c-1063-4fb6-d3a6-76ac71c896aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(inputs['input_values']) , inputs['input_values'][0].shape"
      ],
      "metadata": {
        "id": "eHf2R8eW5JQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71568de3-30ca-4598-a4b3-999cc6b3c933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2880, (32000,))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "Dz6gYbd3pJnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Buidling"
      ],
      "metadata": {
        "id": "7NzEN57rvpeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFWav2Vec2Model\n",
        "\n",
        "\n",
        "def mean_pool(hidden_states, feature_lengths):\n",
        "    attenion_mask = tf.sequence_mask(\n",
        "        feature_lengths, maxlen=MAX_FRAMES, dtype=tf.dtypes.int64\n",
        "    )\n",
        "    padding_mask = tf.cast(\n",
        "        tf.reverse(tf.cumsum(tf.reverse(attenion_mask, [-1]), -1), [-1]),\n",
        "        dtype=tf.dtypes.bool,\n",
        "    )\n",
        "    hidden_states = tf.where(\n",
        "        tf.broadcast_to(\n",
        "            tf.expand_dims(~padding_mask, -1), (BATCH_SIZE, MAX_FRAMES, HIDDEN_DIM)\n",
        "        ),\n",
        "        0.0,\n",
        "        hidden_states,\n",
        "    )\n",
        "    pooled_state = tf.math.reduce_sum(hidden_states, axis=1) / tf.reshape(\n",
        "        tf.math.reduce_sum(tf.cast(padding_mask, dtype=tf.dtypes.float32), axis=1),\n",
        "        [-1, 1],\n",
        "    )\n",
        "    return pooled_state\n",
        "\n",
        "\n",
        "class TFWav2Vec2ForAudioClassification(layers.Layer):\n",
        "\n",
        "    def __init__(self, model_checkpoint, num_classes):\n",
        "        super().__init__()\n",
        "        # Instantiate the Wav2Vec 2.0 model without the Classification-Head\n",
        "        self.wav2vec2 = TFWav2Vec2Model.from_pretrained(\n",
        "            model_checkpoint, apply_spec_augment=False, from_pt=True\n",
        "        )\n",
        "        self.pooling = layers.GlobalAveragePooling1D()\n",
        "        self.intermediate_layer_dropout = layers.Dropout(0.5)\n",
        "        # Classification-Head\n",
        "        self.final_layer = layers.Dense(num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        hidden_states = self.wav2vec2(inputs[0])[0]\n",
        "        if tf.is_tensor(inputs[1]):\n",
        "            audio_lengths = tf.cumsum(inputs[1], -1)[:, -1]\n",
        "            feature_lengths = self.wav2vec2.wav2vec2._get_feat_extract_output_lengths(\n",
        "                audio_lengths\n",
        "            )\n",
        "            pooled_state = mean_pool(hidden_states, feature_lengths)\n",
        "        else:\n",
        "            pooled_state = self.pooling(hidden_states)\n",
        "\n",
        "        intermediate_state = self.intermediate_layer_dropout(pooled_state)\n",
        "        final_state = self.final_layer(intermediate_state)\n",
        "\n",
        "        return final_state"
      ],
      "metadata": {
        "id": "16KdtYJM50Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    # Model's input\n",
        "    inputs = [tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=\"float32\"),\n",
        "         tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=\"int32\"),\n",
        "    ]\n",
        "    wav2vec2_model = TFWav2Vec2ForAudioClassification(MODEL_CHECKPOINT, NUM_CLASSES)(\n",
        "        inputs\n",
        "    )\n",
        "    # Model\n",
        "    model = tf.keras.Model(inputs, wav2vec2_model)\n",
        "    # Loss\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "    # Optimizer\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    # Compile and return\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520,
          "referenced_widgets": [
            "00f358dbbd2f44b9aef40d69fddbefa1",
            "045ed57ca15646c3b280c2d8b1160516",
            "02e0f876c7e142d6a40fc1feedf170b5",
            "28bfc6f85fe748d7abc47d7552ced244",
            "57c44d2ffee841f8b20d61fd0bf1fa92",
            "12d72777557d436cbaff00efc07bca6e",
            "ccee9901a9d549d2ade6f7ba2d6540c3",
            "ff10dc490ac44294adc10e981920e975",
            "623e1171e4be41ad856bd6e99c372b0d",
            "c4cc8c8105744ae1bf70fad08d152782",
            "61edb7628cbe464499b354d5fd2f47e5"
          ]
        },
        "id": "MfdyEEaK6A9T",
        "outputId": "a9dcd2a1-479d-4a1c-f7c1-a79d83023e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00f358dbbd2f44b9aef40d69fddbefa1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFWav2Vec2Model: ['quantizer.weight_proj.weight', 'project_hid.weight', 'quantizer.weight_proj.bias', 'project_q.bias', 'quantizer.codevectors', 'project_q.weight', 'project_hid.bias']\n",
            "- This IS expected if you are initializing TFWav2Vec2Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFWav2Vec2Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFWav2Vec2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWav2Vec2Model for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32000)]      0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 32000)]      0           []                               \n",
            "                                                                                                  \n",
            " tf_wav2_vec2_for_audio_classif  (2, 8)              94377864    ['input_1[0][0]',                \n",
            " ication (TFWav2Vec2ForAudioCla                                   'input_2[0][0]']                \n",
            " ssification)                                                                                     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 94,377,864\n",
            "Trainable params: 94,377,864\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = [y for x, y in inputs.items()]"
      ],
      "metadata": {
        "id": "BydIzm4b8CkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tx = np.array(train_x)\n",
        "tx.shape"
      ],
      "metadata": {
        "id": "cyaEbiw6AzFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c76fa690-192d-4697-ad8c-537998ef638a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 2880, 32000)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = np.zeros((y.size, y.max() + 1))\n",
        "b[np.arange(y.size), y] = 1\n",
        "b.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_QzcxwHktEj",
        "outputId": "d9d8e0f9-9d8e-4db9-b5b3-66f912adcab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2880, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    [tx[0],tx[1]],\n",
        "    b,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=MAX_EPOCHS,\n",
        ")"
      ],
      "metadata": {
        "id": "0nExvolW8CGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5b458a-45a2-4182-ac39-58f954ccb5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_wav2_vec2_model_1/wav2vec2/masked_spec_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_wav2_vec2_model_1/wav2vec2/masked_spec_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_wav2_vec2_model_1/wav2vec2/masked_spec_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_wav2_vec2_model_1/wav2vec2/masked_spec_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1440/1440 [==============================] - 203s 108ms/step - loss: 2.1089 - accuracy: 0.1340\n",
            "Epoch 2/5\n",
            "1440/1440 [==============================] - 155s 107ms/step - loss: 2.0138 - accuracy: 0.1726\n",
            "Epoch 3/5\n",
            "1440/1440 [==============================] - 155s 108ms/step - loss: 1.3923 - accuracy: 0.4837\n",
            "Epoch 4/5\n",
            "1440/1440 [==============================] - 166s 115ms/step - loss: 0.7201 - accuracy: 0.7486\n",
            "Epoch 5/5\n",
            "1440/1440 [==============================] - 168s 116ms/step - loss: 0.3382 - accuracy: 0.8865\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff2fc571f60>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and loading the model using inbuilt functions of keras_core"
      ],
      "metadata": {
        "id": "rYr1Xx99vt3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDUDDao6i6C3",
        "outputId": "9ee1315d-da05-4529-c653-2c5d541744f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.pooling.global_average_pooling1d.GlobalAveragePooling1D object at 0x7ff2fcefe740>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as dropout_101_layer_call_fn, dropout_101_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, feature_extractor_layer_call_fn while saving (showing 5 of 423). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = keras.models.load_model('model/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GTY_z2SoTOj",
        "outputId": "5a24f6eb-8a3b-48af-8a32-ba6145b81a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from transformers import AutoFeatureExtractor\n",
        "\n",
        "MAX_DURATION = 2\n",
        "# Sampling rate is the number of samples of audio recorded every second\n",
        "SAMPLING_RATE = 16000\n",
        "BATCH_SIZE = 2  # Batch-size for training and evaluating our model.\n",
        "NUM_CLASSES = 8  # Number of classes our dataset will have (11 in our case).\n",
        "HIDDEN_DIM = 768  # Dimension of our model output (768 in case of Wav2Vec 2.0 - Base).\n",
        "MAX_SEQ_LENGTH = MAX_DURATION * SAMPLING_RATE  # Maximum length of the input audio file.\n",
        "# Wav2Vec 2.0 results in an output frequency with a stride of about 20ms.\n",
        "MAX_FRAMES = 99\n",
        "MAX_EPOCHS = 5  # Maximum number of training epochs.\n",
        "RAVDESS_CLASS_LABELS = (\"angry\", \"calm\", \"disgust\", \"fear\", \"happy\", \"neutral\",\"sad\",\"surprise\")\n",
        "MODEL_CHECKPOINT = \"facebook/wav2vec2-base\"\n",
        "\n",
        "labels = RAVDESS_CLASS_LABELS\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label\n",
        "\n",
        "print(id2label)\n",
        "\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "    MODEL_CHECKPOINT, return_attention_mask=True\n",
        ")\n",
        "tmp = keras.models.load_model('model/')\n",
        "sample,_ = librosa.load('/content/ravdess/Actor_01/03-01-02-01-01-02-01.wav',res_type='kaiser_fast')\n",
        "inp =  feature_extractor(\n",
        "    sample[:64000],\n",
        "    sampling_rate=feature_extractor.sampling_rate,\n",
        "    max_length=MAX_SEQ_LENGTH,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        ")\n",
        "inp = np.array([y for x,y in inp.items()])\n",
        "\n",
        "print(inp[0])\n",
        "pred = tmp.predict([inp[0],inp[1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHW3hYSexGMK",
        "outputId": "ce1d6d3e-a85d-4a5c-860b-535c18c31b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 6.13512413e-04  6.13512413e-04  6.13512413e-04 ... -7.35014141e-01\n",
            "  -8.40578854e-01 -9.34421659e-01]]\n",
            "1/1 [==============================] - 5s 5s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id2label[str(np.argmax(pred))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WI3s48-axo2d",
        "outputId": "7de3434d-8a8d-4c0f-fa8b-e28d8da6bd3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'calm'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio demo creation"
      ],
      "metadata": {
        "id": "Vd9UqKv5v39R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "r3PThVarsKMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from transformers import AutoFeatureExtractor\n",
        "\n",
        "MAX_DURATION = 2\n",
        "# Sampling rate is the number of samples of audio recorded every second\n",
        "SAMPLING_RATE = 16000\n",
        "BATCH_SIZE = 2  # Batch-size for training and evaluating our model.\n",
        "NUM_CLASSES = 8  # Number of classes our dataset will have (11 in our case).\n",
        "HIDDEN_DIM = 768  # Dimension of our model output (768 in case of Wav2Vec 2.0 - Base).\n",
        "MAX_SEQ_LENGTH = MAX_DURATION * SAMPLING_RATE  # Maximum length of the input audio file.\n",
        "# Wav2Vec 2.0 results in an output frequency with a stride of about 20ms.\n",
        "MAX_FRAMES = 99\n",
        "MAX_EPOCHS = 5  # Maximum number of training epochs.\n",
        "RAVDESS_CLASS_LABELS = (\"angry\", \"calm\", \"disgust\", \"fear\", \"happy\", \"neutral\",\"sad\",\"surprise\")\n",
        "MODEL_CHECKPOINT = \"facebook/wav2vec2-base\"\n",
        "\n",
        "labels = RAVDESS_CLASS_LABELS\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label\n",
        "\n",
        "\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "    MODEL_CHECKPOINT, return_attention_mask=True\n",
        ")\n",
        "tmp = keras.models.load_model('model/')\n",
        "\n",
        "def greet(name):\n",
        "  inp =  feature_extractor(\n",
        "    name[1],\n",
        "    sampling_rate=feature_extractor.sampling_rate,\n",
        "    max_length=MAX_SEQ_LENGTH,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "  )\n",
        "  inp = np.array([y for x,y in inp.items()])\n",
        "  pred = tmp.predict([inp[0],inp[1]])\n",
        "  lab = id2label[str(np.argmax(pred))]\n",
        "  return lab\n",
        "\n",
        "iface = gr.Interface(fn=greet, inputs=\"audio\", outputs=\"text\")\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "2x727vK8sGiS",
        "outputId": "84f057ca-fe04-4ac6-d316-d3048f35e04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7861, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q intel-tensorflow keras==2.12.0 tensorboard==2.12 tensorflow-estimator==2.12.0"
      ],
      "metadata": {
        "id": "JVlPe6Y7vfWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa83285e-8d12-499f-8142-363616a071ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.7/244.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
        "!export ONEDNN_VERBOSE=1\n",
        "!export KMP_SETTINGS=TRUE"
      ],
      "metadata": {
        "id": "IpWr6iDovN7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "NS7IfSmPxkwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFWav2Vec2Model\n",
        "\n",
        "\n",
        "def mean_pool(hidden_states, feature_lengths):\n",
        "    attenion_mask = tf.sequence_mask(\n",
        "        feature_lengths, maxlen=MAX_FRAMES, dtype=tf.dtypes.int64\n",
        "    )\n",
        "    padding_mask = tf.cast(\n",
        "        tf.reverse(tf.cumsum(tf.reverse(attenion_mask, [-1]), -1), [-1]),\n",
        "        dtype=tf.dtypes.bool,\n",
        "    )\n",
        "    hidden_states = tf.where(\n",
        "        tf.broadcast_to(\n",
        "            tf.expand_dims(~padding_mask, -1), (BATCH_SIZE, MAX_FRAMES, HIDDEN_DIM)\n",
        "        ),\n",
        "        0.0,\n",
        "        hidden_states,\n",
        "    )\n",
        "    pooled_state = tf.math.reduce_sum(hidden_states, axis=1) / tf.reshape(\n",
        "        tf.math.reduce_sum(tf.cast(padding_mask, dtype=tf.dtypes.float32), axis=1),\n",
        "        [-1, 1],\n",
        "    )\n",
        "    return pooled_state\n",
        "\n",
        "\n",
        "class TFWav2Vec2ForAudioClassification(layers.Layer):\n",
        "\n",
        "    def __init__(self, model_checkpoint, num_classes):\n",
        "        super().__init__()\n",
        "        # Instantiate the Wav2Vec 2.0 model without the Classification-Head\n",
        "        self.wav2vec2 = TFWav2Vec2Model.from_pretrained(\n",
        "            model_checkpoint, apply_spec_augment=False, from_pt=True\n",
        "        )\n",
        "        self.pooling = layers.GlobalAveragePooling1D()\n",
        "        self.intermediate_layer_dropout = layers.Dropout(0.5)\n",
        "        # Classification-Head\n",
        "        self.final_layer = layers.Dense(num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        hidden_states = self.wav2vec2(inputs[0])[0]\n",
        "        if tf.is_tensor(inputs[1]):\n",
        "            audio_lengths = tf.cumsum(inputs[1], -1)[:, -1]\n",
        "            feature_lengths = self.wav2vec2.wav2vec2._get_feat_extract_output_lengths(\n",
        "                audio_lengths\n",
        "            )\n",
        "            pooled_state = mean_pool(hidden_states, feature_lengths)\n",
        "        else:\n",
        "            pooled_state = self.pooling(hidden_states)\n",
        "\n",
        "        intermediate_state = self.intermediate_layer_dropout(pooled_state)\n",
        "        final_state = self.final_layer(intermediate_state)\n",
        "\n",
        "        return final_state"
      ],
      "metadata": {
        "id": "OeHJm_7qxkwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    # Model's input\n",
        "    inputs = [tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=\"float32\"),\n",
        "         tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=\"int32\"),\n",
        "    ]\n",
        "    wav2vec2_model = TFWav2Vec2ForAudioClassification(MODEL_CHECKPOINT, NUM_CLASSES)(\n",
        "        inputs\n",
        "    )\n",
        "    # Model\n",
        "    model = tf.keras.Model(inputs, wav2vec2_model)\n",
        "    # Loss\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "    # Optimizer\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    # Compile and return\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e93234c0-a709-4827-f9f6-236da3cb4e1c",
        "id": "elMJoq6nxkwk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "\n",
            "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFWav2Vec2Model: ['quantizer.weight_proj.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'project_hid.weight', 'quantizer.codevectors', 'project_q.weight', 'project_q.bias']\n",
            "- This IS expected if you are initializing TFWav2Vec2Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFWav2Vec2Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFWav2Vec2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWav2Vec2Model for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32000)]      0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 32000)]      0           []                               \n",
            "                                                                                                  \n",
            " tf_wav2_vec2_for_audio_classif  (2, 8)              94377864    ['input_1[0][0]',                \n",
            " ication (TFWav2Vec2ForAudioCla                                   'input_2[0][0]']                \n",
            " ssification)                                                                                     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 94,377,864\n",
            "Trainable params: 6,152\n",
            "Non-trainable params: 94,371,712\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = [y for x, y in inputs.items()]"
      ],
      "metadata": {
        "id": "CZ7viRQTxkwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tx = np.array(train_x)\n",
        "tx.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a25b9289-1d56-43d4-961e-0f3d96f3a33c",
        "id": "LDxOHXyExkwk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 2880, 32000)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = np.zeros((y.size, y.max() + 1))\n",
        "b[np.arange(y.size), y] = 1\n",
        "b.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd5724d-f2eb-4515-cdae-3a611bb45f77",
        "id": "B7cfUi8Dxkwl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2880, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    [tx[0],tx[1]],\n",
        "    b,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=MAX_EPOCHS,\n",
        ")"
      ],
      "metadata": {
        "id": "AE0J0PiCxkwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rReX7iJhxnOI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}